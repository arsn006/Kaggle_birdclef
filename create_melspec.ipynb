{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8CLPy8xk8XE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21576,
     "status": "ok",
     "timestamp": 1622344780106,
     "user": {
      "displayName": "中村有紗",
      "photoUrl": "",
      "userId": "01679400299700621239"
     },
     "user_tz": -540
    },
    "id": "AKxPDfkWk9i9",
    "outputId": "b7190a49-77b9-47ac-dc55-95ba28007464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1622344786550,
     "user": {
      "displayName": "中村有紗",
      "photoUrl": "",
      "userId": "01679400299700621239"
     },
     "user_tz": -540
    },
    "id": "a4glk9PRlB8Q",
    "outputId": "924ebfb6-c9bf-4373-99e2-bf6cf32e9e7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/bird2/code\n"
     ]
    }
   ],
   "source": [
    "cd \"/content/drive/MyDrive/bird2/code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "executionInfo": {
     "elapsed": 6755,
     "status": "ok",
     "timestamp": 1622344794340,
     "user": {
      "displayName": "中村有紗",
      "photoUrl": "",
      "userId": "01679400299700621239"
     },
     "user_tz": -540
    },
    "id": "Emr6T6D0Z8AD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Global vars\n",
    "RANDOM_SEED = 1337\n",
    "SAMPLE_RATE = 32000\n",
    "SIGNAL_LENGTH = 7 # seconds\n",
    "SPEC_SHAPE = (128, 256) # height x width\n",
    "#SPEC_SHAPE = (48, 128) # height x width\n",
    "FMIN = 500\n",
    "FMAX = 12500\n",
    "MAX_AUDIO_FILES = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 214,
     "status": "ok",
     "timestamp": 1622344797260,
     "user": {
      "displayName": "中村有紗",
      "photoUrl": "",
      "userId": "01679400299700621239"
     },
     "user_tz": -540
    },
    "id": "NvwsrEeibGfS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1982,
     "status": "ok",
     "timestamp": 1622344799526,
     "user": {
      "displayName": "中村有紗",
      "photoUrl": "",
      "userId": "01679400299700621239"
     },
     "user_tz": -540
    },
    "id": "HAaLd_A2Z8AE",
    "outputId": "3416711d-4dac-4b9b-aa1d-eb9bdd03f23f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF SPECIES IN TRAIN DATA: 397\n",
      "NUMBER OF SAMPLES IN TRAIN DATA: 59386\n",
      "LABELS: ['acafly', 'acowoo', 'aldfly', 'ameavo', 'amecro', 'amegfi', 'amekes', 'amepip', 'amered', 'amerob', 'amewig', 'amtspa', 'andsol1', 'annhum', 'astfly', 'azaspi1', 'babwar', 'baleag', 'balori', 'banana', 'banswa', 'banwre1', 'barant1', 'barswa', 'batpig1', 'bawswa1', 'bawwar', 'baywre1', 'bbwduc', 'bcnher', 'belkin1', 'belvir', 'bewwre', 'bkbmag1', 'bkbplo', 'bkbwar', 'bkcchi', 'bkhgro', 'bkmtou1', 'bknsti', 'blbgra1', 'blbthr1', 'blcjay1', 'blctan1', 'blhpar1', 'blkpho', 'blsspa1', 'blugrb1', 'blujay', 'bncfly', 'bnhcow', 'bobfly1', 'bongul', 'botgra', 'brbmot1', 'brbsol1', 'brcvir1', 'brebla', 'brncre', 'brnjay', 'brnthr', 'brratt1', 'brwhaw', 'brwpar1', 'btbwar', 'btnwar', 'btywar', 'bucmot2', 'buggna', 'bugtan', 'buhvir', 'bulori', 'burwar1', 'bushti', 'butsal1', 'buwtea', 'cacgoo1', 'cacwre', 'calqua', 'caltow', 'cangoo', 'canwar', 'carchi', 'carwre', 'casfin', 'caskin', 'caster1', 'casvir', 'categr', 'ccbfin', 'cedwax', 'chbant1', 'chbchi', 'chbwre1', 'chcant2', 'chispa', 'chswar', 'cinfly2', 'clanut', 'clcrob', 'cliswa', 'cobtan1', 'cocwoo1', 'cogdov', 'colcha1', 'coltro1', 'comgol', 'comgra', 'comloo', 'commer', 'compau', 'compot1', 'comrav', 'comyel', 'coohaw', 'cotfly1', 'cowscj1', 'cregua1', 'creoro1', 'crfpar', 'cubthr', 'daejun', 'dowwoo', 'ducfly', 'dusfly', 'easblu', 'easkin', 'easmea', 'easpho', 'eastow', 'eawpew', 'eletro', 'eucdov', 'eursta', 'fepowl', 'fiespa', 'flrtan1', 'foxspa', 'gadwal', 'gamqua', 'gartro1', 'gbbgul', 'gbwwre1', 'gcrwar', 'gilwoo', 'gnttow', 'gnwtea', 'gocfly1', 'gockin', 'gocspa', 'goftyr1', 'gohque1', 'goowoo1', 'grasal1', 'grbani', 'grbher3', 'grcfly', 'greegr', 'grekis', 'grepew', 'grethr1', 'gretin1', 'greyel', 'grhcha1', 'grhowl', 'grnher', 'grnjay', 'grtgra', 'grycat', 'gryhaw2', 'gwfgoo', 'haiwoo', 'heptan', 'hergul', 'herthr', 'herwar', 'higmot1', 'hofwoo1', 'houfin', 'houspa', 'houwre', 'hutvir', 'incdov', 'indbun', 'kebtou1', 'killde', 'labwoo', 'larspa', 'laufal1', 'laugul', 'lazbun', 'leafly', 'leasan', 'lesgol', 'lesgre1', 'lesvio1', 'linspa', 'linwoo1', 'littin1', 'lobdow', 'lobgna5', 'logshr', 'lotduc', 'lotman1', 'lucwar', 'macwar', 'magwar', 'mallar3', 'marwre', 'mastro1', 'meapar', 'melbla1', 'monoro1', 'mouchi', 'moudov', 'mouela1', 'mouqua', 'mouwar', 'mutswa', 'naswar', 'norcar', 'norfli', 'normoc', 'norpar', 'norsho', 'norwat', 'nrwswa', 'nutwoo', 'oaktit', 'obnthr1', 'ocbfly1', 'oliwoo1', 'olsfly', 'orbeup1', 'orbspa1', 'orcpar', 'orcwar', 'orfpar', 'osprey', 'ovenbi1', 'pabspi1', 'paltan1', 'palwar', 'pasfly', 'pavpig2', 'phivir', 'pibgre', 'pilwoo', 'pinsis', 'pirfly1', 'plawre1', 'plaxen1', 'plsvir', 'plupig2', 'prowar', 'purfin', 'purgal2', 'putfru1', 'pygnut', 'rawwre1', 'rcatan1', 'rebnut', 'rebsap', 'rebwoo', 'redcro', 'reevir1', 'rehbar1', 'relpar', 'reshaw', 'rethaw', 'rewbla', 'ribgul', 'rinkin1', 'roahaw', 'robgro', 'rocpig', 'rotbec', 'royter1', 'rthhum', 'rtlhum', 'ruboro1', 'rubpep1', 'rubrob', 'rubwre1', 'ruckin', 'rucspa1', 'rucwar', 'rucwar1', 'rudpig', 'rudtur', 'rufhum', 'rugdov', 'rumfly1', 'runwre1', 'rutjac1', 'saffin', 'sancra', 'sander', 'savspa', 'saypho', 'scamac1', 'scatan', 'scbwre1', 'scptyr1', 'scrtan1', 'semplo', 'shicow', 'sibtan2', 'sinwre1', 'sltred', 'smbani', 'snogoo', 'sobtyr1', 'socfly1', 'solsan', 'sonspa', 'soulap1', 'sposan', 'spotow', 'spvear1', 'squcuc1', 'stbori', 'stejay', 'sthant1', 'sthwoo1', 'strcuc1', 'strfly1', 'strsal1', 'stvhum2', 'subfly', 'sumtan', 'swaspa', 'swathr', 'tenwar', 'thbeup1', 'thbkin', 'thswar1', 'towsol', 'treswa', 'trogna1', 'trokin', 'tromoc', 'tropar', 'tropew1', 'tuftit', 'tunswa', 'veery', 'verdin', 'vigswa', 'warvir', 'wbwwre1', 'webwoo1', 'wegspa1', 'wesant1', 'wesblu', 'weskin', 'wesmea', 'westan', 'wewpew', 'whbman1', 'whbnut', 'whcpar', 'whcsee1', 'whcspa', 'whevir', 'whfpar1', 'whimbr', 'whiwre1', 'whtdov', 'whtspa', 'whwbec1', 'whwdov', 'wilfly', 'willet1', 'wilsni1', 'wiltur', 'wlswar', 'wooduc', 'woothr', 'wrenti', 'y00475', 'yebcha', 'yebela1', 'yebfly', 'yebori1', 'yebsap', 'yebsee1', 'yefgra1', 'yegvir', 'yehbla', 'yehcar1', 'yelgro', 'yelwar', 'yeofly1', 'yerwar', 'yeteup1', 'yetvir']\n"
     ]
    }
   ],
   "source": [
    "# Code adapted from: \n",
    "# https://www.kaggle.com/frlemarchand/bird-song-classification-using-an-efficientnet\n",
    "# Make sure to check out the entire notebook.\n",
    "\n",
    "# Load metadata file\n",
    "train = pd.read_csv('../input/birdclef-2021/train_metadata.csv',)\n",
    "#train = pd.read_csv('./birdclef-2021/train_metadata.csv',)\n",
    "\n",
    "# Limit the number of training samples and classes\n",
    "# First, only use high quality samples\n",
    "train = train.query('rating>=1')\n",
    "\n",
    "# Second, assume that birds with the most training samples are also the most common\n",
    "# A species needs at least 200 recordings with a rating above 4 to be considered common\n",
    "birds_count = {}\n",
    "for bird_species, count in zip(train.primary_label.unique(), \n",
    "                               train.groupby('primary_label')['primary_label'].count().values):\n",
    "    birds_count[bird_species] = count\n",
    "most_represented_birds = [key for key,value in birds_count.items() if value >= 0] \n",
    "\n",
    "TRAIN = train.query('primary_label in @most_represented_birds')\n",
    "LABELS = sorted(TRAIN.primary_label.unique())\n",
    "\n",
    "# Let's see how many species and samples we have left\n",
    "print('NUMBER OF SPECIES IN TRAIN DATA:', len(LABELS))\n",
    "print('NUMBER OF SAMPLES IN TRAIN DATA:', len(TRAIN))\n",
    "print('LABELS:', most_represented_birds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1622345065341,
     "user": {
      "displayName": "中村有紗",
      "photoUrl": "",
      "userId": "01679400299700621239"
     },
     "user_tz": -540
    },
    "id": "eKoGJIfASygm"
   },
   "outputs": [],
   "source": [
    "class AudioTransform:\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        self.always_apply = always_apply\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        if self.always_apply:\n",
    "            return self.apply(y)\n",
    "        else:\n",
    "            if np.random.rand() < self.p:\n",
    "                return self.apply(y)\n",
    "            else:\n",
    "                return y\n",
    "\n",
    "    def apply(self, y: np.ndarray):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        for trns in self.transforms:\n",
    "            y = trns(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class OneOf:\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        n_trns = len(self.transforms)\n",
    "        trns_idx = np.random.choice(n_trns)\n",
    "        trns = self.transforms[trns_idx]\n",
    "        return trns(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1622345066299,
     "user": {
      "displayName": "中村有紗",
      "photoUrl": "",
      "userId": "01679400299700621239"
     },
     "user_tz": -540
    },
    "id": "RTEsrN0QSyOg"
   },
   "outputs": [],
   "source": [
    "class AddGaussianNoise(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, max_noise_amplitude=0.5, **kwargs):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.noise_amplitude = (0.0, max_noise_amplitude)\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        noise_amplitude = np.random.uniform(*self.noise_amplitude)\n",
    "        noise = np.random.randn(len(y))\n",
    "        augmented = (y + noise * noise_amplitude).astype(y.dtype)\n",
    "        return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1622345067087,
     "user": {
      "displayName": "中村有紗",
      "photoUrl": "",
      "userId": "01679400299700621239"
     },
     "user_tz": -540
    },
    "id": "j4HGRI37SyIO"
   },
   "outputs": [],
   "source": [
    "class GaussianNoiseSNR(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5.0, max_snr=20.0, **kwargs):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y ** 2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        white_noise = np.random.randn(len(y))\n",
    "        a_white = np.sqrt(white_noise ** 2).max()\n",
    "        augmented = (y + white_noise * 1 / a_white * a_noise).astype(y.dtype)\n",
    "        return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5141,
     "status": "ok",
     "timestamp": 1622345073033,
     "user": {
      "displayName": "中村有紗",
      "photoUrl": "",
      "userId": "01679400299700621239"
     },
     "user_tz": -540
    },
    "id": "K9YdCyCDSyBr",
    "outputId": "4289629c-8b68-444f-ae8c-f6a1d8d2bb96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting colorednoise\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/3e/85645bcaa5ba6003c6e3c650fe23c6352f7aa4a36eb1d700f3609e52963e/colorednoise-1.1.1.tar.gz\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from colorednoise) (1.19.5)\n",
      "Building wheels for collected packages: colorednoise\n",
      "  Building wheel for colorednoise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for colorednoise: filename=colorednoise-1.1.1-cp37-none-any.whl size=3958 sha256=08d68b36801f9b6a1b77b8a11a90072a24f13af4d0b06bfa5c592ef0985d7782\n",
      "  Stored in directory: /root/.cache/pip/wheels/84/be/f3/3e7e1c80ebab3f6f0dbd3e34e787b902d2280d66706485fef4\n",
      "Successfully built colorednoise\n",
      "Installing collected packages: colorednoise\n",
      "Successfully installed colorednoise-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install colorednoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1622345075160,
     "user": {
      "displayName": "中村有紗",
      "photoUrl": "",
      "userId": "01679400299700621239"
     },
     "user_tz": -540
    },
    "id": "6HCP8h-PSx72"
   },
   "outputs": [],
   "source": [
    "import colorednoise as cn\n",
    "\n",
    "\n",
    "class PinkNoiseSNR(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5.0, max_snr=20.0, **kwargs):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y ** 2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        pink_noise = cn.powerlaw_psd_gaussian(1, len(y))\n",
    "        a_pink = np.sqrt(pink_noise ** 2).max()\n",
    "        augmented = (y + pink_noise * 1 / a_pink * a_noise).astype(y.dtype)\n",
    "        return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1622345076155,
     "user": {
      "displayName": "中村有紗",
      "photoUrl": "",
      "userId": "01679400299700621239"
     },
     "user_tz": -540
    },
    "id": "KJICfQ_PSx2G"
   },
   "outputs": [],
   "source": [
    "class PitchShift(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, max_steps=5, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.max_steps = max_steps\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        n_steps = np.random.randint(-self.max_steps, self.max_steps)\n",
    "        augmented = librosa.effects.pitch_shift(y, sr=self.sr, n_steps=n_steps)\n",
    "        return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1622345076970,
     "user": {
      "displayName": "中村有紗",
      "photoUrl": "",
      "userId": "01679400299700621239"
     },
     "user_tz": -540
    },
    "id": "3riWfEnXTFYp"
   },
   "outputs": [],
   "source": [
    "class TimeStretch(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, max_rate=1.2):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.max_rate = max_rate\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        rate = np.random.uniform(0, self.max_rate)\n",
    "        augmented = librosa.effects.time_stretch(y, rate)\n",
    "        return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1622345077735,
     "user": {
      "displayName": "中村有紗",
      "photoUrl": "",
      "userId": "01679400299700621239"
     },
     "user_tz": -540
    },
    "id": "jYyywsUsTFOe"
   },
   "outputs": [],
   "source": [
    "class TimeShift(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, max_shift_second=2, sr=32000, padding_mode=\"replace\"):\n",
    "        super().__init__(always_apply, p)\n",
    "    \n",
    "        assert padding_mode in [\"replace\", \"zero\"], \"`padding_mode` must be either 'replace' or 'zero'\"\n",
    "        self.max_shift_second = max_shift_second\n",
    "        self.sr = sr\n",
    "        self.padding_mode = padding_mode\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        shift = np.random.randint(-self.sr * self.max_shift_second, self.sr * self.max_shift_second)\n",
    "        augmented = np.roll(y, shift)\n",
    "        if self.padding_mode == \"zero\":\n",
    "            if shift > 0:\n",
    "                augmented[:shift] = 0\n",
    "            else:\n",
    "                augmented[shift:] = 0\n",
    "        return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1622345078550,
     "user": {
      "displayName": "中村有紗",
      "photoUrl": "",
      "userId": "01679400299700621239"
     },
     "user_tz": -540
    },
    "id": "N_WTxoKPTJiC"
   },
   "outputs": [],
   "source": [
    "class VolumeControl(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, db_limit=10, mode=\"uniform\"):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        assert mode in [\"uniform\", \"fade\", \"fade\", \"cosine\", \"sine\"], \\\n",
    "            \"`mode` must be one of 'uniform', 'fade', 'cosine', 'sine'\"\n",
    "\n",
    "        self.db_limit= db_limit\n",
    "        self.mode = mode\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        db = np.random.uniform(-self.db_limit, self.db_limit)\n",
    "        if self.mode == \"uniform\":\n",
    "            db_translated = 10 ** (db / 20)\n",
    "        elif self.mode == \"fade\":\n",
    "            lin = np.arange(len(y))[::-1] / (len(y) - 1)\n",
    "            db_translated = 10 ** (db * lin / 20)\n",
    "        elif self.mode == \"cosine\":\n",
    "            cosine = np.cos(np.arange(len(y)) / len(y) * np.pi * 2)\n",
    "            db_translated = 10 ** (db * cosine / 20)\n",
    "        else:\n",
    "            sine = np.sin(np.arange(len(y)) / len(y) * np.pi * 2)\n",
    "            db_translated = 10 ** (db * sine / 20)\n",
    "        augmented = y * db_translated\n",
    "        return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 207,
     "status": "ok",
     "timestamp": 1622345160573,
     "user": {
      "displayName": "中村有紗",
      "photoUrl": "",
      "userId": "01679400299700621239"
     },
     "user_tz": -540
    },
    "id": "6D1K0v-4Tj3g"
   },
   "outputs": [],
   "source": [
    "def mono_to_color(X, eps=1e-6, mean=None, std=None):\n",
    "    mean = mean or X.mean()\n",
    "    std = std or X.std()\n",
    "    X = (X - mean) / (std + eps)\n",
    "    \n",
    "    _min, _max = X.min(), X.max()\n",
    "\n",
    "    if (_max - _min) > eps:\n",
    "        V = np.clip(X, _min, _max)\n",
    "        V = 255 * (V - _min) / (_max - _min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        V = np.zeros_like(X, dtype=np.uint8)\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1622345161782,
     "user": {
      "displayName": "中村有紗",
      "photoUrl": "",
      "userId": "01679400299700621239"
     },
     "user_tz": -540
    },
    "id": "F03l5kthZ8AF",
    "outputId": "af518f74-d35e-46b5-b2c3-09a3e803de3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL NUMBER OF AUDIO FILES IN TRAINING DATA: 59386\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the training data and limit the number of audio files to MAX_AUDIO_FILES\n",
    "#TRAIN = shuffle(TRAIN, random_state=RANDOM_SEED)[:MAX_AUDIO_FILES]\n",
    "#TRAIN = shuffle(TRAIN, random_state=RANDOM_SEED)\n",
    "\n",
    "# Define a function that splits an audio file, \n",
    "# extracts spectrograms and saves them in a working directory\n",
    "def get_spectrograms(filepath, primary_label, output_dir):\n",
    "    \n",
    "    # Open the file with librosa (limited to the first 15 seconds)\n",
    "    sig, rate = librosa.load(filepath, sr=SAMPLE_RATE, offset=None, duration=15)\n",
    "    \n",
    "    # Split signal into five second chunks\n",
    "    sig_splits = []\n",
    "    for i in range(0, len(sig), int(SIGNAL_LENGTH * SAMPLE_RATE)):\n",
    "        split = sig[i:i + int(SIGNAL_LENGTH * SAMPLE_RATE)]\n",
    "\n",
    "        # End of signal?\n",
    "        if len(split) < int(SIGNAL_LENGTH * SAMPLE_RATE):\n",
    "            break\n",
    "        \n",
    "        sig_splits.append(split)\n",
    "        \n",
    "    # Extract mel spectrograms for each audio chunk\n",
    "    s_cnt = 0\n",
    "    saved_samples = []\n",
    "    for chunk in sig_splits:\n",
    "        transform_gaussian = GaussianNoiseSNR(always_apply=True, min_snr=5, max_snr=20)\n",
    "        y_gaussian_snr = transform_gaussian(chunk)\n",
    "        transform_pink = PinkNoiseSNR(always_apply=True, min_snr=5.0, max_snr=20.0)\n",
    "        y_pink_noise = transform_pink(y_gaussian_snr)\n",
    "        transform_pitch = PitchShift(max_steps=2, sr=SAMPLE_RATE)\n",
    "        y_pitch_shift = transform_pitch(y_pink_noise)\n",
    "        transform_time = TimeShift(sr=SAMPLE_RATE)\n",
    "        y_time_shift = transform_time(y_pitch_shift)\n",
    "        \n",
    "        \n",
    "        hop_length = int(SIGNAL_LENGTH * SAMPLE_RATE / (SPEC_SHAPE[1] - 1))\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y_time_shift, \n",
    "                                                  sr=SAMPLE_RATE, \n",
    "                                                  n_fft=1024, \n",
    "                                                  hop_length=hop_length, \n",
    "                                                  n_mels=SPEC_SHAPE[0], \n",
    "                                                  fmin=FMIN, \n",
    "                                                  fmax=FMAX)\n",
    "    \n",
    "        #mel_spec = librosa.power_to_db(mel_spec, ref=np.max) \n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max).astype(np.float32)\n",
    "        image = mono_to_color(mel_spec)\n",
    "        # # Normalize\n",
    "        # mel_spec -= mel_spec.min()\n",
    "        # mel_spec /= mel_spec.max()\n",
    "        \n",
    "        # Save as image file\n",
    "        save_dir = os.path.join(output_dir, primary_label)\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        save_path = os.path.join(save_dir, filepath.rsplit(os.sep, 1)[-1].rsplit('.', 1)[0] + \n",
    "                                 '_' + str(s_cnt) + '.npy')\n",
    "        np.save(str(save_path), image)\n",
    "        # save_path = os.path.join(save_dir, filepath.rsplit(os.sep, 1)[-1].rsplit('.', 1)[0] + \n",
    "        #                          '_' + str(s_cnt) + '.png')\n",
    "        # im = Image.fromarray(mel_spec * 255.0).convert(\"L\")\n",
    "        # im.save(save_path)\n",
    "        \n",
    "        saved_samples.append(save_path)\n",
    "        s_cnt += 1\n",
    "        \n",
    "        \n",
    "    return saved_samples\n",
    "\n",
    "print('FINAL NUMBER OF AUDIO FILES IN TRAINING DATA:', len(TRAIN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFQVToBZZ8AH",
    "outputId": "28a475c1-445e-4db9-c97c-cbbdcbb9e257"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 17991/59386 [4:54:58<11:14:45,  1.02it/s]"
     ]
    }
   ],
   "source": [
    "# Parse audio files and extract training samples\n",
    "input_dir = '../input/birdclef-2021/train_short_audio/'\n",
    "#input_dir = './birdclef-2021/train_short_audio/'\n",
    "#output_dir = '../working/melspectrogram_dataset/'\n",
    "output_dir = '../input/melspectrogram_dataset/'\n",
    "samples = []\n",
    "with tqdm(total=len(TRAIN)) as pbar:\n",
    "    for idx, row in TRAIN.iterrows():\n",
    "        pbar.update(1)\n",
    "        \n",
    "        if row.primary_label in most_represented_birds:\n",
    "            audio_file_path = os.path.join(input_dir, row.primary_label, row.filename)\n",
    "            samples += get_spectrograms(audio_file_path, row.primary_label, output_dir)\n",
    "            \n",
    "TRAIN_SPECS = shuffle(samples, random_state=RANDOM_SEED)\n",
    "print('SUCCESSFULLY EXTRACTED {} SPECTROGRAMS'.format(len(TRAIN_SPECS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3R9-YTWZ8AM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "birdclef2021-model-training.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
